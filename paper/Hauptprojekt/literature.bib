@article{Viola2001,
	abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "integral image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Viola, P. and Jones, M.},
	doi = {10.1109/CVPR.2001.990517},
	eprint = {arXiv:1011.1669v3},
	file = {:Users/johannesberger/Downloads/TR2004-043.pdf:pdf},
	isbn = {0-7695-1272-0},
	issn = {1063-6919},
	journal = {Cvpr},
	mendeley-groups = {Hauptprojekt},
	pages = {I--511--I--518},
	pmid = {7143246},
	title = {{Haar-like}},
	url = {http://ieeexplore.ieee.org/document/990517/},
	volume = {1},
	year = {2001}
}
@article{Adnan2013,
	abstract = {Advanced technology offers us various alternatives for collecting traffic data. However, different devices often result in different accuracy to the true speed of the drivers. Lack of knowledge of accuracy between different devices is often cited as a common problem for both transportation researcher and practitioner. This paper discusses the most accurate traffic data measurement device when compared to the true driving speed of the driver using the V-BOX GPS validated with the dash box of the test vehicle. The paper illustrates and discusses the significant value R{\textless}sup{\textgreater}2{\textless}/sup{\textgreater} of the traffic data using scatter plot, root mean squared error (RMSE), mean absolute error (MAE) and mean absolute percentage error (MAPE). The paper covers two classes of advanced traffic data collection devices which are intrusive (automated traffic classifier) and off road portable speed measurement devices (laser gun, radar gun and manual count). Results showed that automated traffic classifier have the smaller discrepancies or deviations followed by laser gun, manual count and radar gun when comparing to the global positioning system (GPS). It is extremely important to notify which devices have the most accurate data collection as any study can only be as accurate as the data on which it is based.},
	author = {Adnan, Muhammad Akram and Sulaiman, Norliana and Zainuddin, Nor Izzah and Besar, Tuan Badrul Hisyam Tuan},
	doi = {10.1109/BEIAC.2013.6560214},
	file = {:Users/johannesberger/Dropbox/uni/WS18/Hauptprojekt/Papers/AdnanTuanBesar-2013-VehicleSpeedMeasurementTechniqueUsingVariousSpeedDetectionInstrumentation.pdf:pdf},
	isbn = {9781467359689},
	journal = {BEIAC 2013 - 2013 IEEE Business Engineering and Industrial Applications Colloquium},
	keywords = {Speed measurements,traffic data collection},
	mendeley-groups = {Hauptprojekt},
	number = {June 2015},
	pages = {668--672},
	title = {{Vehicle speed measurement technique using various speed detection instrumentation}},
	year = {2013}
}
@misc{OpenCV2016,
	author = {OpenCV},
	mendeley-groups = {Hauptprojekt},
	title = {{OpenCV: Camera Calibration and 3D Reconstruction}},
	url = {http://docs.opencv.org/master/d9/d0c/group{\_}{\_}calib3d.html{\#}ga549c2075fac14829ff4a58bc931c033d},
	urldate = {16.6.2019},
	year = {2016}
}
@article{Redmon2017,
	abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1612.08242v1},
	author = {Redmon, Joseph and Farhadi, Ali},
	doi = {10.1109/CVPR.2017.690},
	eprint = {arXiv:1612.08242v1},
	file = {:Users/johannesberger/Dropbox/uni/WS18/Hauptprojekt/Papers/1612.08242.pdf:pdf},
	isbn = {9781538604571},
	journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
	mendeley-groups = {Hauptprojekt},
	pages = {6517--6525},
	title = {{YOLO9000: Better, faster, stronger}},
	volume = {2017-Janua},
	year = {2017}
}
@article{Berger2018,
	author = {Berger, Johannes},
	file = {:Users/johannesberger/Dropbox/uni/WS18/Grundprojekt/LaTeX/Template/grundProjekt.pdf:pdf},
	keywords = {automatische kennzeichenerkennung,convolutional neural network,yolo},
	mendeley-groups = {Hauptprojekt},
	title = {{Grundprojekt: Building a Prototype for Real-Time License Plate Extraction using Deep Learning}},
	year = {2018}
}
@misc{yolov3,
	abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
	archivePrefix = {arXiv},
	arxivId = {1804.02767},
	author = {Redmon, Joseph and Farhadi, Ali},
	booktitle = {arXiv},
	doi = {10.1109/CVPR.2017.690},
	eprint = {1804.02767},
	file = {:Users/johannesberger/Library/Application Support/Mendeley Desktop/Downloaded/Redmon, Farhadi - 2018 - YOLOv3 An Incremental Improvement.pdf:pdf},
	isbn = {978-1-5386-0457-1},
	issn = {0146-4833},
	mendeley-groups = {Grundprojekt,Grundprojekt/Yolo,Hauptprojekt},
	pmid = {23021419},
	title = {{YOLOv3: An Incremental Improvement}},
	url = {http://arxiv.org/abs/1804.02767},
	year = {2018}
}
@article{He2016,
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1512.03385v1},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	doi = {10.1109/CVPR.2016.90},
	eprint = {arXiv:1512.03385v1},
	file = {:Users/johannesberger/Dropbox/uni/WS18/Hauptprojekt/Papers/1512.03385.pdf:pdf},
	isbn = {9781467388504},
	issn = {10636919},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	mendeley-groups = {Hauptprojekt},
	pages = {770--778},
	title = {{Deep residual learning for image recognition}},
	volume = {2016-Decem},
	year = {2016}
}
@article{Spanhel2017,
	abstract = {{\textcopyright} 2017 IEEE. This work is focused on recognition of license plates in low resolution and low quality images. We present a methodology for collection of real world (non-synthetic) dataset of low quality license plate images with ground truth transcriptions. Our approach to the license plate recognition is based on a Convolutional Neural Network which holistically processes the whole image, avoiding segmentation of the license plate characters. Evaluation results on multiple datasets show that our method significantly outperforms other free and commercial solutions to license plate recognition on the low quality data. To enable further research of low quality license plate recognition, we make the datasets publicly available.},
	author = {Spanhel, Jakub and Sochor, Jakub and Juranek, Roman and Herout, Adam and Marsik, Lukas and Zemcik, Pavel},
	doi = {10.1109/AVSS.2017.8078501},
	file = {:Users/johannesberger/Library/Application Support/Mendeley Desktop/Downloaded/Spanhel et al. - 2017 - Holistic recognition of low quality license plates by CNN using track annotated data.pdf:pdf},
	isbn = {9781538629390},
	journal = {2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance, AVSS 2017},
	mendeley-groups = {Hauptprojekt},
	number = {August},
	title = {{Holistic recognition of low quality license plates by CNN using track annotated data}},
	year = {2017}
}
@article{Svoboda2016,
	abstract = {In this work we explore the previously proposed approach of direct blind deconvolution and denoising with convolutional neural networks in a situation where the blur kernels are partially constrained. We focus on blurred images from a real-life traffic surveillance system, on which we, for the first time, demonstrate that neural networks trained on artificial data provide superior reconstruction quality on real images compared to traditional blind deconvolution methods. The training data is easy to obtain by blurring sharp photos from a target system with a very rough approximation of the expected blur kernels, thereby allowing custom CNNs to be trained for a specific application (image content and blur range). Additionally, we evaluate the behavior and limits of the CNNs with respect to blur direction range and length.},
	archivePrefix = {arXiv},
	arxivId = {1602.07873},
	author = {Svoboda, Pavel and Hradis, Michal and Marsik, Lukas and Zemcik, Pavel},
	doi = {10.1109/ICIP.2016.7533077},
	eprint = {1602.07873},
	file = {:Users/johannesberger/Library/Application Support/Mendeley Desktop/Downloaded/Svoboda et al. - 2016 - CNN for license plate motion deblurring.pdf:pdf},
	isbn = {9781467399616},
	issn = {15224880},
	journal = {Proceedings - International Conference on Image Processing, ICIP},
	keywords = {Blind deconvolution,Convolutional neural network,Image reconstruction,License plate,Motion blur},
	mendeley-groups = {Hauptprojekt},
	pages = {3832--3836},
	pmid = {30202859},
	title = {{CNN for license plate motion deblurring}},
	volume = {2016-Augus},
	year = {2016}
}
@article{Redmon,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1506.02640v5},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	eprint = {arXiv:1506.02640v5},
	file = {:Users/johannesberger/Library/Application Support/Mendeley Desktop/Downloaded/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Detection.pdf:pdf},
	mendeley-groups = {Grundprojekt,Grundprojekt/Yolo,Hauptprojekt},
	title = {{You Only Look Once: Unified, Real-Time Object Detection}},
	year = {2016}
}
